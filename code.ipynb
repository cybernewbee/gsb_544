{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "\n",
    "### 1. Data Import and Inspection\n",
    "1. import\\\n",
    "df = pd.read_csv('fullpath')\\\n",
    "print(df.head())\\\n",
    "print(df.info())\n",
    "2. round()\n",
    "3. creat a list\\\n",
    "row = []\\\n",
    "row[0]\n",
    "4. creat am array\\\n",
    "y = np.array([1,2,3,4,5])\\\n",
    "y[2]\\\n",
    "- if different data types are put in, python will convert it\n",
    "5. creat matrix; same data type only\\\n",
    "matrix([[1,2,3], [4,5,6]])\n",
    "6. dataframe\n",
    "- list of array with same length; same type in one column, but different types in different column\\\n",
    "df.body_mass_g # to call a column\\\n",
    "df.body_mass_g.mean \n",
    "\n",
    "### 2. Data Cleaning and Transformation\n",
    "\n",
    "1. Rename columns\\\n",
    "df = df.rename(columns={'old': 'new_name'})\n",
    "\n",
    "2. Fill Nas .fillna()\\\n",
    "vector = vector.fillna(\"\")\n",
    "\n",
    "3. Change data types   as.type()\\\n",
    "df['column'] = df['column'].astype('float')\\\n",
    "df['column'] = df['column'].astype('str')\n",
    "\n",
    "4. Convert a column to datetime with a custom format and extract parts pd.to_datetime()  \\\n",
    "df['date'] = pd.to_datetime(df['date'], format='%B %d, %Y')\\\n",
    "df['year'] = df['date'].dt.year\\\n",
    "df['month'] = df['date'].dt.month\\\n",
    "df['day'] = df['date'].dt.day\n",
    "\n",
    "5. Ensure Series type\\\n",
    "day = pd.Series(day)\n",
    "\n",
    "### 3. String operation\n",
    "\n",
    "1. Remove extra whitespace from string data\\\n",
    "phrase = phrase.strip()\n",
    "2. Starts with a letter\\\n",
    "mask = data.str.startswith(('A', 'E', 'I', 'O', 'U', 'a', 'e', 'i', 'o', 'u'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 4. Data Selection and Filtering\n",
    "\n",
    "1. Select specific columns entirly\\\n",
    "df_name[\"column\"]\\\n",
    "df_name.loc[:, \"age\"]\\\n",
    "df_name.loc[:, [\"age\", \"fare\"]]\\\n",
    "df_name[[\"age\", \"fare\"]]\\\n",
    "df.name.age\\\n",
    "\n",
    "\n",
    "2. Filter rows based on criteria\\\n",
    "df_titanic.iloc[1, 3]\\\n",
    "df_name[df_name[\"column\"] > 30]\n",
    "df_name[df_name[\"column\"] == criteria]\\\n",
    "df_name[(df_name[\"column\"] > 10) & (df_name[\"column2\"] == \"value\")]\\\n",
    "df_name[(df_name[\"column\"] > 10) | (df_name[\"column2\"] == \"value\")]\n",
    "\n",
    "- Use 'isin' to filter rows\n",
    "df_2 = df_1[df_1['col1'].isin(criteria_list)]\\\n",
    "relax_days = isin(weekdays(data), weekend(if))\\\n",
    "weekdays[relax_days] \\\n",
    "- Using ~ to reverse the True and False\\\n",
    "weekdays[~relax_days]\\\n",
    "mask_reg1 = big_coffee['state'].isin(['Connecticut', 'Maine','Massachusetts',\n",
    "                                      'New Hampshire', 'Rhode Island',\n",
    "                                      'Vermont', 'New Jersey', 'New York',\n",
    "                                      'Pennsylvania'])\\\n",
    "big_coffee.loc[mask_reg1, 'region'] = 'Northeast'\n",
    "\n",
    "3. Sorting\\\n",
    "df_name = df_name.sort_values(by=[\"age\"], ascending=False)\n",
    "\n",
    "4. Mutating\\\n",
    "df_titanic[\"age\"] = df_titanic[\"age\"] / 10 # divided by 10\\\n",
    "df_titanic[\"fare\"] = np.log(df_titanic[\"fare\"]) # log \\\n",
    "df_titanic[\"nonsense\"] = df_titanic[\"fare\"] / df_titanic[\"age\"] \\\n",
    "- assigning new value to a new column \n",
    "penguins.loc[:, \"X_sq\"] = penguins['bill_length_mm']**2\n",
    "- Add a dummy variable\\\n",
    "df_titanic[\"female\"] = 1 * (df_titanic[\"gender\"] == \"female\")\\\n",
    "pd.get_dummies(df_titanic[\"embarked\"])\n",
    "\n",
    "\n",
    "5. Summarizing Rows\n",
    "df_titanic[\"age\"].mean()\\\n",
    "df_titanic[\"age\"].median()\\\n",
    "df_titanic[[\"age\", \"fare\"]].mean()\n",
    "df[df[\"column\"] == criteria1].describe(); can put in mean, std, or empty\n",
    "- Groupby with stats\\\n",
    "df_titanic[[\"age\", \"survived\"]].groupby(\"survived\").mean()\n",
    "- Find unique\\\n",
    "df_titanic[\"class\"].unique()\\\n",
    "df_titanic[\"class\"].describe()\n",
    "- count value by category\\\n",
    "df_titanic[\"class\"].value_counts()\\\n",
    "df_titanic[\"class\"].value_counts(normalize=True)\n",
    "- joint distribution\n",
    "pd.crosstab(df_titanic[\"survived\"], df_titanic[\"gender\"])\\\n",
    "pd.crosstab(df_titanic[\"survived\"], df_titanic[\"gender\"], normalize=True)\n",
    "- conditional probability\\\n",
    "marginal_embarked = joint_distribution.sum(axis=0)\\\n",
    "print(marginal_embarked)\\\n",
    "conditional_class_given_emabrked = joint_distribution.div(marginal_embarked, axis = 1) # div by rows\\\n",
    "print(conditional_class_given_emabrked)\\\n",
    "marginal_class = joint_distribution.sum(axis=1)\\\n",
    "print(marginal_class)\\\n",
    "conditional_emabraked_given_class = joint_distribution.div(marginal_class, axis = 0) # div by columns\\\n",
    "print(conditional_emabraked_given_class)\n",
    "- Group by and aggregate with multiple functions\\\n",
    "agg_result = df.groupby('geography').agg(\n",
    "    mean_total_volume=('total_volume', 'mean'),\n",
    "    sum_total_bags=('total_bags', 'sum'),\n",
    "    max_average_price=('average_price', 'max')\n",
    ")\n",
    "\n",
    "6. Create a new categorical variable\\\n",
    "df_titanic[\"age_cat\"] = pd.cut(df_titanic[\"age\"],\n",
    "                              bins = [0, 18, 100],\n",
    "                              labels = [\"child\", \"adult\"])\n",
    "\n",
    "\n",
    "7. Reorder categorical data with custom ordering\\\n",
    "df_name['size'] = pd.Categorical(df_name['column'], categories=[\"1\", \"2\", \"3\"], ordered=True)\n",
    "\n",
    "8. Change the levels of a categorical variable\\\n",
    "df_titanic[\"type\"] = df_titanic[\"class\"].map({\n",
    "    \"1st\": \"passenger\",\n",
    "    \"2nd\": \"passenger\",\n",
    "    \"3rd\": \"passenger\",\n",
    "    \"victualling crew\": \"crew\",\n",
    "    \"engineering crew\": \"crew\",\n",
    "    \"deck crew\": \"crew\"\n",
    "})\\\n",
    "df_titanic\n",
    "\n",
    "9. Convert an index as a column\\\n",
    "df[\"newcolumn\"] = df.index\n",
    "\n",
    "### 5. Data Joining and Pivoting\n",
    "1. Reshape wide data to long format\\\n",
    "df_long = df.melt(id_vars=[\"country\", \"continent\"], value_vars=['2005', '2006', '2007', '2008'], var_name='year', value_name='population')\\\n",
    "long_population = population.melt(id_vars=[\"country\", \"continent\"], var_name=\"year\", value_name=\"population\")\n",
    "\n",
    "2. Reshape long data to wide format\\\n",
    "wide_df = long_df.pivot(index=[\"country\", \"continent\"], columns=\"year\", values=\"population\")\\\n",
    "wide_df = wide_df.reset_index()\n",
    "- long_population.pivot(): The pivot() function reshapes the data from long to wide format.\n",
    "- index=\"country\": This specifies the column to use as the row index in the wide format. In this case, each country will form a row.\n",
    "- columns=\"year\": This determines what should become the column headers. In this case, each unique value in the year column becomes a new column.\n",
    "- values=\"population\": These are the values to place in the new wide-format DataFrame. The population values from the long format will fill the corresponding cells in the wide format.\n",
    "\n",
    "3. Joining two DataFrames on specified columns\\\n",
    "df_merged = df.merge(df2, on=[\"col1\", \"col2\", \"col3\"])\n",
    "- assign new suffixes after joining\\\n",
    "names1995.merge(names2015, on=[\"Name\", \"Sex\"], suffixes=(\"1995\", \"2015\"))\\\n",
    "names1995.merge(\\\n",
    "    names2015_,\\\n",
    "    left_on=(\"Name\", \"Sex\"),\\\n",
    "    right_on=(\"Name\", \"Gender\")\\\n",
    ")\\\n",
    "- inner join\\\n",
    "names_inner = names1995.merge(names2005, on=[\"Name\", \"Sex\"], how=\"inner\")\n",
    "- outer join\\\n",
    "names_outer = names1995.merge(names2005, on=[\"Name\", \"Sex\"], how=\"outer\")\n",
    "- left join\\\n",
    "names_left = names1995.merge(names2005, on=[\"Name\", \"Sex\"], how=\"left\")\n",
    "\n",
    "4. Concatenation\\\n",
    "pd.concat([names1995, names2015])\\\n",
    "- adding a column to diff two table\\\n",
    "names1995[\"Year\"] = 1995\\\n",
    "names2015[\"Year\"] = 2015\\\n",
    "names = pd.concat([names1995, names2015], ignore_index=True)\n",
    "\n",
    "\n",
    "### 4. Logical Operations\n",
    "\n",
    "1. np.where()\\\n",
    "data = np.where(criteria, modify_if_yes, modify_if_no)\n",
    "\n",
    "2. nested .where()\\\n",
    "data = np.where(criteria, np.where(criteria, modify_if_yes, modify_if_no), modify_if_no)\n",
    "\n",
    "3. .any() \\\n",
    "if data.any():\\\n",
    "    data\\\n",
    "else \"\"\\\n",
    "\n",
    "4. for\\\n",
    "for fruit in fruits:\n",
    "\n",
    "### 5. Iteration\n",
    "- for example\\\n",
    "song = \"\"\\\n",
    "for i in range(100,97,-1):\\\n",
    "  song = song + sing_verse(i)\\\n",
    "print(song)\n",
    "- The if statement needs either a single True or a single False to determine if the subsequent code will be run\\\n",
    "a_vec = np.array([-2, 1, -3, -9, 7])\\\n",
    "for val in a_vec:\\\n",
    "  if val > 0:\\\n",
    "    val = np.sqrt(val)\\\n",
    "  print(val)\n",
    "- The map() function requires the same information as a for loop: what values we want to iterate over, and what we want to do with each value.\\\n",
    "song = map(sing_verse, range(100, 97, -1))\\\n",
    "song = list(song)\\\n",
    "print(\"\".join(song))\n",
    "- Lambda functions\\\n",
    "song = map(lambda i: sing_verse_2(i, \"lemonade\"), nums)\\\n",
    "print(\"\".join(list(song)))\\\n",
    "- Apply and lambda \\\n",
    "lands_name = df.apply(lambda x: re.findall(r\"[Lland]\", str(x)))\\\n",
    "song = dat.apply(lambda x: sing_verse_3(x['num'], x['drink'], x['container']), axis=1)\n",
    "- Apply with entire df with no direction meaning it applies to every cell\\ \n",
    "dat.apply(np.sqrt)\n",
    "-  Apply with rows\\\n",
    "dat.apply(np.sum, axis=1)\n",
    "\n",
    "\n",
    "### 6. Data Viz \n",
    "from plotnine import\\\n",
    "1. boxplot \n",
    "(ggplot(penguins, aes(x = \"species\", y = \"bill_length_mm\", fill = \"species\")) + geom_boxplot()\n",
    ")\n",
    "2. bar chart with additional variable as fill\\\n",
    "(ggplot(penguins,aes(x = \"species\", fill = \"sex\")) + geom_bar(stat=\"identity\",position = \"fill\"))\\\n",
    "geom_bar(position = \"dodge\")\n",
    "3. histograms with bin size\\\n",
    "(ggplot(penguins, aes(x = \"bill_length_mm\" ))+ geom_histogram(bins = 100))\n",
    "4. density with alpha\\\n",
    "(ggplot(penguins, aes(x = \"bill_length_mm\", fill = \"species\" ))+ geom_density(alpha = 0.5))\n",
    "5. scatterplot with color\\\n",
    "(ggplot(penguins,aes(x = \"bill_length_mm\", y = \"bill_depth_mm\", color = \"species\"))+ geom_point())\n",
    "6. lineplot with a group column\\\n",
    "(ggplot(penguins2, aes(x = \"species\", y = \"bill_length_mm\", color = \"sex\", group = \"sex\"\\\n",
    "))+ geom_point()+ geom_line())\n",
    "7. jitter plot\\\n",
    "[p9.ggplot(df3) +\n",
    " p9.aes(x = \"four_regions\", y= \"internet_users \",fill = \"four_regions\", size = \"income\") +\n",
    " p9.geom_jitter(width=0.2, height=0.2, alpha = 0.8)\n",
    "]\n",
    "9. Create a DataFrame with predictions for the regression line; pa 6.1\\\n",
    "penguins['predicted_bill_depth'] = lr_fit_1.predict(X)\\\n",
    "\n",
    "Plot with plotnine\n",
    "(p9.ggplot(penguins, p9.aes(x='bill_length_mm', y='bill_depth_mm')) +\n",
    "p9.geom_point(alpha=0.6, color='blue') +  # Scatter plot of actual data\n",
    "p9.geom_line(p9.aes(y='predicted_bill_depth'), color='red', size=1.2) +  # Regression line\n",
    "p9.labs(\n",
    "        x='Bill Length (mm)',\n",
    "        y='Bill Depth (mm)',\n",
    "        title='Linear Regression Model'\n",
    "    )   \n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "8. argument\\\n",
    "coord_flip()\n",
    "scale_y_reverse()\\\n",
    "scale_x_continuous()\\\n",
    "scale_x_discrete()\\\n",
    "scale_size(range=(2, 15))\n",
    "scale_x_log10(breaks = [500, 1000, 2000, 4000, 8000, 16000, 32000, 64000])\\\n",
    "guides(size = \"none\")\n",
    "theme_classic()\\\n",
    "facet_wrap(\"sex\")\\\n",
    "labs(x = \"Income\", y =\"Life Expectancy\")\\\n",
    "ylim(0, 100)\\\n",
    ".annotate(\"text\", x = 10000, y = 60, label = \"2010\", size = 190, color = \"black\", alpha =0.2)\\\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. regular expression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. String element\n",
    "- string_1[0:5]\n",
    "- string_1[5:]\n",
    "- string_2[-12:-1]\n",
    "### 2.  Turn my whole string into lowercase\n",
    "- string_1.lower()\n",
    "### 3. Clean space\n",
    "string_3.strip()\n",
    "### 4. Placeholder with format()\n",
    "- string_4 = \"My name is {lastname}, {firstname} {lastname}.\"\n",
    "- string_4.format(firstname = \"James\", lastname = \"Baldwin\")\n",
    "### 5. Splitting\n",
    "- fish_list = fish_string.split(\", \")\n",
    "### 6. Search a string using regular expressions\n",
    "- re.findall(r\"[Mr]oses\", moses_string)\n",
    "### 7. Wildcards\n",
    "- \\b: “boundary” between word and non-word, such as punctuation or whitespace\n",
    "- \\s : “space” matches a single whitespace character\n",
    "- \\d : “digit” matches any single number 0-9\n",
    "- ^ : matches the start of a string\n",
    "- $ : matches the end of a string\n",
    "- . : matches any character at all, except a new line (\\n)\n",
    "-  \"+\" character means “match the previous thing at least one time”\n",
    "- \"*\" : Match 0 or more of the previous character\n",
    "- ? : Match 0 or one of the previous character\n",
    "- {2} : Match exactly two of the previous character\n",
    "- {1,3}: Match 1 to 3 repeats of the previous character\n",
    "- \\w regular expression shortcut to say “match any character that might be found in a word\n",
    "- '\\1y' the end word with y\n",
    "- 'v\\1' the first word with v\n",
    "- [^\\w\\s] all punctuations\n",
    "### 8. Look behind\n",
    "- moses_string = \"Moses parted the Red Sea. Moses climbed the mountain.\"\n",
    "matches = re.findall(r\"(?<=Moses )\\w+\", moses_string)\n",
    "print(matches)\n",
    "['parted', 'climbed']\n",
    "### 9. Look ahead\n",
    "re.findall(r\"\\w+(?= Moses )\", moses_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Penalized Regresion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Loss Function\n",
    "- lambda to control the hashness\n",
    "- beta large = overfit\n",
    "- beta small = underfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Net\n",
    "- alpha (λ): The overall regularization strength. Higher values increase regularization.\n",
    "- l1_ratio (ρ): The proportion of L1 regularization relative to L2 regularization.\n",
    "- l1_ratio = 0 corresponds to pure Ridge regression.\n",
    "- l1_ratio = 1 corresponds to pure Lasso regression.\n",
    "- 0 < l1_ratio < 1 combines both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- L1 Regularization (Lasso):\n",
    "- Encourages sparsity in the model, leading to feature selection by shrinking some coefficients to exactly zero.\n",
    "- L2 Regularization (Ridge):\n",
    "- Penalizes large coefficients to reduce overfitting, but does not force any coefficients to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Grid Search\n",
    "- total computation = #param1 X #param2 X cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Precision of class I: \"if I say yes, am I right?\"\n",
    "- precsion = trup positive / total positive\n",
    "- tp / (tp + fp)\n",
    "### 2. Recall of class I : \"How many did I find?\"\n",
    "- recall = true positive / actual positive\n",
    "- tp / (tp + fn) \n",
    "### 3. Specificity \n",
    "- Specificity = true negative / actual negative\n",
    "- tn / (tn + fp)\n",
    "### 4. F1 score\n",
    "- F1 = 2 * (precision * recall) / (precision * recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Non-linear Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Multi Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Tree (non para)\n",
    "- min_sample_leaf = number of obervation in the \"last\" bin; controlling how deep the tree is; deep = overfitting; shallow = underfitting\n",
    "- test error is not linear; number of leaf:1 10 100 high low high \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. KNN (non para)\n",
    "- k "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Quadrict Discriminant Analysis (QDA)\n",
    "- Curvy version of LDA\n",
    "- reg_param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Support Vector Machine (SVM)\n",
    "- kernal: poly to raise dimensions\n",
    "- tune: C [0 , 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Logistic Regression\n",
    "- OLS does not work due to dummy 1 and 0\n",
    "- z = b + b1x1 + b2x2 + bpxp\n",
    "- if z is computed, \n",
    "- then p = 1 / (1 + np.exp(-z))\n",
    "- Decision boundry whens p = 0.5 and log(p/1-p) = 0\n",
    "- 0 = b + b1x1 + b2x2 + bpxp\n",
    "- if p is given\n",
    "- then x2 = (np.log(p/(1-p)) - intercept - b1 * x1) / b2   \n",
    "- Full process see PA 9.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Linear Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Multi Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Linear Discriminant Analysis (LDA)\n",
    "- Draw a line to get the most different normal distribution\n",
    "- 0 = w1x1 + w2x2 + b \n",
    "- x2 = -(w1 * x1 + b) / w2; no scale\n",
    "- If scaled, \n",
    "- get std, mean for both x1 and x2. Turn x1 to x1_scaled.\n",
    "- plug x1_scaled in formula. Receive x2_scaled\n",
    "- x2 = x2_scaled * std_chol + mean_chol\n",
    "- See PA 9.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Support Vector Classifier (SVC)\n",
    "- Max the margin between then two lines\n",
    "- 0 = w1x1 + w2x2 + b \n",
    "- c [0,1]\n",
    "- the rest is the same as LDA\n",
    "- See PA 9.1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
